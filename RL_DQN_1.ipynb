{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPX8qD2VrAi3cD5OLRRf15B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anower120/AI-900-AIFundamentals/blob/main/RL_DQN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The code sets up a RL loan approval system using Gym toolkit for developing and comparing RL algorithms.\n",
        "# This environment simulates the process of approving or denying loan applications based on borrower data.\n",
        "###\n",
        "#Import the libraries\n",
        "# Importing Required Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class LoanEnv(gym.Env):\n",
        "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, borrower_data):\n",
        "        super(LoanEnv, self).__init__()\n",
        "        self.borrower_data = borrower_data\n",
        "        # Define action and observation space\n",
        "        # They must be gym.spaces objects\n",
        "        self.action_space = spaces.Discrete(2)  # Approve or Deny\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(len(borrower_data[0]),), dtype=np.float32)\n",
        "\n",
        "        # Example for internal state\n",
        "        self.state = None\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the state of the environment to an initial state\n",
        "        self.state = self.borrower_data[0]  # Example of resetting to the first borrower\n",
        "        return np.array(self.state, dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Execute one time step within the environment\n",
        "        # Simplified reward logic\n",
        "        reward = 0\n",
        "        done = False\n",
        "        info = {}\n",
        "\n",
        "        # Example logic for moving to the next borrower in the dataset\n",
        "        next_state = self.state # Placeholder for actual logic\n",
        "        reward = 1 if action == 0 else -1  # Placeholder for actual logic\n",
        "\n",
        "        # Check if we're done with the dataset\n",
        "        done = True  # Placeholder for actual logic\n",
        "\n",
        "        return np.array(next_state, dtype=np.float32), reward, done, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        # Render the environment to the screen\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "# Sample Borrower Data\n",
        "borrower_data = np.array([\n",
        "    [750, 60000, 10000, 10],\n",
        "    [650, 40000, 5000, 5],\n",
        "    [500, 30000, 15000, 2]\n",
        "])\n",
        "\n",
        "# Normalize the Data\n",
        "scaler = MinMaxScaler()\n",
        "borrower_data_normalized = scaler.fit_transform(borrower_data)\n",
        "\n",
        "# Instantiate the LoanEnv with normalized borrower data\n",
        "env = LoanEnv(borrower_data_normalized)\n",
        "\n",
        "\n",
        "# Define the DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize DQN Agent\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "\n",
        "# Training the DQN Agent\n",
        "# This step involves running episodes of interactions with the environment,\n",
        "# storing experiences in a replay buffer, and periodically training the agent on a batch of experiences.\n",
        "num_episodes = 100  # Total episodes for training\n",
        "batch_size = 32  # Batch size for training the agent\n",
        "\n",
        "# Training the DQN Agent\n",
        "for e in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    total_reward = 0\n",
        "\n",
        "    for time in range(500):  # Maximum time steps per episode\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            print(f\"Episode: {e+1}/{num_episodes}, Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
        "            break\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)\n",
        "\n",
        "\n",
        "# Save the model after training\n",
        "agent.save(\"loan_dqn_model.h5\")\n",
        "\n",
        "# Load the model for evaluation or further training\n",
        "agent.load(\"loan_dqn_model.h5\")\n",
        "\n",
        "\n",
        "# Evaluating the DQN Model\n",
        "num_test_episodes = 100  # Adjusted for demonstration\n",
        "total_rewards = 0\n",
        "\n",
        "for e in range(num_test_episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    episode_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action = np.argmax(agent.model.predict(state)[0])\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = np.reshape(next_state, [1, state_size])\n",
        "        episode_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    total_rewards += episode_reward\n",
        "    print(f\"Test Episode: {e+1}, Reward: {episode_reward}\")\n",
        "\n",
        "average_reward = total_rewards / num_test_episodes\n",
        "print(f\"Average Reward Over {num_test_episodes} Test Episodes: {average_reward}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Woy7FXbMNhYL",
        "outputId": "d7e09fe2-af70-4e71-96ce-5b2ee81c6576"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 2/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 3/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 4/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 5/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 6/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 7/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 8/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 9/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 10/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 11/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 12/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 13/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 14/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 15/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 16/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 17/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 18/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 19/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 20/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 21/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 22/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 23/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 24/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 25/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 26/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 27/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 28/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 29/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 30/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 31/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 32/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 33/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 34/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 35/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 36/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 37/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 38/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 39/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 40/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 41/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 42/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 43/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 44/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 45/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 46/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 47/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 48/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 49/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 50/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 51/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 52/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 53/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 54/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 55/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 56/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 57/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 58/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 59/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 60/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 61/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 62/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 63/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 64/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 65/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 66/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 67/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 68/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 69/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 70/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 71/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 72/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 73/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 74/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 75/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 76/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 77/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 78/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 79/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 80/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 81/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 82/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 83/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 84/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 85/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 86/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 87/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 88/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 89/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 90/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 91/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 92/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 93/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 94/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 95/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 96/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 97/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 98/100, Reward: 1, Epsilon: 1.00\n",
            "Episode: 99/100, Reward: -1, Epsilon: 1.00\n",
            "Episode: 100/100, Reward: 1, Epsilon: 1.00\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "Test Episode: 1, Reward: -1\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Test Episode: 2, Reward: -1\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Test Episode: 3, Reward: -1\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Test Episode: 4, Reward: -1\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Test Episode: 5, Reward: -1\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Test Episode: 6, Reward: -1\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Test Episode: 7, Reward: -1\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Test Episode: 8, Reward: -1\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Test Episode: 9, Reward: -1\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Test Episode: 10, Reward: -1\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Test Episode: 11, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 12, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 13, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 14, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 15, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 16, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 17, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 18, Reward: -1\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Test Episode: 19, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 20, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 21, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 22, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 23, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 24, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 25, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 26, Reward: -1\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Test Episode: 27, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 28, Reward: -1\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Test Episode: 29, Reward: -1\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Test Episode: 30, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 31, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 32, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 33, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 34, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 35, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 36, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 37, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 38, Reward: -1\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Test Episode: 39, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 40, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 41, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 42, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 43, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 44, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 45, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 46, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 47, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 48, Reward: -1\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Test Episode: 49, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 50, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 51, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 52, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 53, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 54, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 55, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 56, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 57, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 58, Reward: -1\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Test Episode: 59, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 60, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 61, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 62, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 63, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 64, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 65, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 66, Reward: -1\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Test Episode: 67, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 68, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 69, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 70, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 71, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 72, Reward: -1\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Test Episode: 73, Reward: -1\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Test Episode: 74, Reward: -1\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Test Episode: 75, Reward: -1\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Test Episode: 76, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 77, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 78, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 79, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 80, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 81, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 82, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 83, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 84, Reward: -1\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Test Episode: 85, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 86, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 87, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 88, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 89, Reward: -1\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Test Episode: 90, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 91, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 92, Reward: -1\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Test Episode: 93, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 94, Reward: -1\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Test Episode: 95, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 96, Reward: -1\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Test Episode: 97, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 98, Reward: -1\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Test Episode: 99, Reward: -1\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Test Episode: 100, Reward: -1\n",
            "Average Reward Over 100 Test Episodes: -1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ieOzhp0zjZWh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}